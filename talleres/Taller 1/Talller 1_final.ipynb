{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0855141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "\n",
    "# Spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType\n",
    "\n",
    "\n",
    "# GeoSpark imports\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "from geospark.sql.types import GeometryType\n",
    "from geospark.utils import GeoSparkKryoRegistrator\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "# set local variables\n",
    "%env MINIO_PASSWORD 6HgSzdwj8eNpHcux\n",
    "%env MINIO_USERNAME grupo-02\n",
    "\n",
    "# parametros para conectar a mongo\n",
    "client = pymongo.MongoClient(\"mongodb+srv://julianestevanof:ql6e7a6B4xGetupj@taller1.joran3x.mongodb.net/?retryWrites=true&w=majority\")\n",
    "db_mongo = client.Taller1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c67e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\n",
    "    \"PYSPARK_SUBMIT_ARGS\"\n",
    "] = \"--packages org.apache.hadoop:hadoop-aws:3.2.2,io.delta:delta-core_2.12:1.1.0  pyspark-shell \"\n",
    "config = {\n",
    "    \"spark.jars.packages\":\"org.apache.hadoop:hadoop-aws:3.2.2\",\n",
    "    \"spark.kubernetes.namespace\": \"spark\",\n",
    "    \"spark.kubernetes.container.image\": \"cronosnull/abd-spark-base:202301\",\n",
    "    \"spark.executor.instances\": \"15\",\n",
    "    \"spark.executor.memory\": \"10g\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.driver.memory\":\"5g\",\n",
    "    \"spark.driver.port\":\"38891\",\n",
    "    \"spark.driver.blockManager.port\":\"7779\",\n",
    "    \"spark.driver.bindAddress\": \"0.0.0.0\",\n",
    "    \"spark.driver.host\":\"172.24.99.147\",\n",
    "    \"spark.kubernetes.executor.request.cores\":\"500m\",\n",
    "    \"spark.hadoop.fs.s3a.endpoint\": \"http://172.24.99.18:9000\",\n",
    "    \n",
    "    # Credenciales de MinNIO, no olvide asignar las variables de entorno\n",
    "    \"spark.hadoop.fs.s3a.access.key\": os.environ.get('MINIO_USERNAME', \"--\"),\n",
    "    \"spark.hadoop.fs.s3a.secret.key\": os.environ.get('MINIO_PASSWORD',\"--\"),\n",
    "    \"spark.hadoop.fs.s3a.path.style.access\": True,\n",
    "    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\": \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "    \"spark.kubernetes.local.dirs.tmpfs\":True,\n",
    "\n",
    "}\n",
    "def get_spark_session(app_name: str, conf: SparkConf):\n",
    "    conf.setMaster(\"k8s://https://172.24.99.68:16443\")\n",
    "    for key, value in config.items():\n",
    "        conf.set(key, value)\n",
    "    conf.set(\"spark.ui.port\", \"4041\");\n",
    "    return SparkSession.builder.appName(app_name).config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3bb386",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark_session(\"grupo-02-taller1\", SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d81a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc540b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869903d",
   "metadata": {},
   "source": [
    "# Cargar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474563f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Leer el primer archivo\n",
    "df_1 = spark.read.csv(\"s3a://ais/2020/AIS_2020*.csv.gz\", header=True)\n",
    "df_1 = df_1.withColumn(\"BaseDateTime\", to_timestamp(\"BaseDateTime\", \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "df_1 = df_1.withColumn('LAT', df_1['LAT'].cast(DoubleType()))\n",
    "df_1 = df_1.withColumn('LON', df_1['LON'].cast(DoubleType()))\n",
    "df_1 = df_1.withColumn('documento', lit(\"AIS_2020\"))\n",
    "# df_1 = df_1.sample(0.00001)\n",
    "\n",
    "\n",
    "# Leer el segundo archivo\n",
    "df_2 = spark.read.csv(\"s3a://ais/2019/AIS_2019*.csv.gz\", header=True)\n",
    "df_2 = df_2.withColumn(\"BaseDateTime\", to_timestamp(\"BaseDateTime\", \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "df_2 = df_2.withColumn('LAT', df_2['LAT'].cast(DoubleType()))\n",
    "df_2 = df_2.withColumn('LON', df_2['LON'].cast(DoubleType()))\n",
    "df_2 = df_2.withColumn('documento', lit(\"AIS_2019\"))\n",
    "# df_2 = df_2.sample(0.00001)\n",
    "\n",
    "\n",
    "# Leer el tercer archivo\n",
    "df_3 = spark.read.csv(\"s3a://ais/2018/AIS_2018*.csv.gz\", header=True)\n",
    "df_3 = df_3.withColumn(\"BaseDateTime\", to_timestamp(\"BaseDateTime\", \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "df_3 = df_3.withColumn('LAT', df_3['LAT'].cast(DoubleType()))\n",
    "df_3 = df_3.withColumn('LON', df_3['LON'].cast(DoubleType()))\n",
    "df_3 = df_3.withColumn('documento', lit(\"AIS_2018\"))\n",
    "# df_3 = df_3.sample(0.00001)\n",
    "\n",
    "\n",
    "# Leer el cuarto archivo\n",
    "df_4 = spark.read.csv(\"s3a://ais/2017/AIS_2017*.csv.gz\", header=True)\n",
    "df_4 = df_4.withColumn(\"BaseDateTime\", to_timestamp(\"BaseDateTime\", \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "df_4 = df_4.withColumn(\"TranscieverClass\", lit(''))\n",
    "df_4 = df_4.withColumn('LAT', df_4['LAT'].cast(DoubleType()))\n",
    "df_4 = df_4.withColumn('LON', df_4['LON'].cast(DoubleType()))\n",
    "df_4 = df_4.withColumn('documento', lit(\"AIS_2017\"))\n",
    "# df_4 = df_4.sample(0.00001)\n",
    "\n",
    "# Unir los cuatro DataFrames\n",
    "df_pruebas = df_1.union(df_2).union(df_3).union(df_4)\n",
    "\n",
    "# Reparticionar el DataFrame en 4 particiones\n",
    "df_pruebas = df_pruebas.repartition(4, 'documento').cache()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "438be796",
   "metadata": {},
   "source": [
    "### Add the State to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear función UDF para obtener estado a partir de latitud y longitud\n",
    "def get_state(lat, lon):\n",
    "    \n",
    "    geolocator = Nominatim(user_agent=\"my_app\")\n",
    "    location = geolocator.reverse(f\"{lat}, {lon}\")\n",
    "    if location is not None:\n",
    "        address = location.raw.get('address')\n",
    "        if address:\n",
    "            state = address.get('state')\n",
    "            if state:\n",
    "                return state\n",
    "    return None\n",
    "\n",
    "# Crear función UDF en Spark\n",
    "get_state_udf = udf(get_state, StringType())\n",
    "\n",
    "# Crear nueva columna con el estado obtenido\n",
    "df_with_state = df_pruebas.withColumn(\"State\", get_state_udf(\"LAT\", \"LON\"))\n",
    "\n",
    "df_with_state.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "560ba5b0",
   "metadata": {},
   "source": [
    "### Add extra information of VesselTypeCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd45735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vessel_type_codes = spark.read.csv(\"s3a://user-data/grupo-02/VesselTypeCodes2018.csv\", header=True,sep=\";\")\n",
    "\n",
    "df_with_vessel = df_with_state.join(df_vessel_type_codes, df_with_state.VesselType == df_vessel_type_codes.VesselType).select(df_with_state['*'], df_vessel_type_codes['*'])\n",
    "\n",
    "df_with_vessel.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f4d0025",
   "metadata": {},
   "source": [
    "### Upload data to mongo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97020ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched = df_with_vessel.withColumn('WEEKDAY', date_format(col(\"BaseDateTime\"), \"EEEE\"))\n",
    "\n",
    "df_enriched = df_enriched.withColumn('DATE', date_format(col(\"BaseDateTime\"), \"yyyy MM dd\"))\n",
    "df_enriched = df_enriched.select(\n",
    "    col(\"State\"),\n",
    "    col(\"WEEKDAY\"),\n",
    "    col(\"DATE\"),\n",
    "    col(\"SOG\"),\n",
    "    col(\"Classification\"),\n",
    "    col(\"Group\"),\n",
    "    col(\"Cargo\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be573c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 1,3 y 4\n",
    "df_1_3_4 = df_enriched.groupby('State','DATE').agg(count(lit(1)).alias('Cant'))\n",
    "\n",
    "df_1_3_4_pandas =df_1_3_4.toPandas()\n",
    "db_mongo.Agrupacion1_3_4.insert_many(df_1_3_4_pandas.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a607cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 2 y 5\n",
    "df_2_5 = df_enriched.groupby('Cargo','WEEKDAY','State','DATE').agg(count(lit(1)).alias('Cant'))\n",
    "\n",
    "df_2_5_pandas =df_2_5.toPandas()\n",
    "db_mongo.Agrupacion2_5.insert_many(df_2_5_pandas.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971390b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 6\n",
    "df_6 = df_enriched.groupby('Group').agg(mean(\"SOG\").alias('Avg Velocity'))\n",
    "\n",
    "df_6_pandas =df_6.toPandas()\n",
    "db_mongo.Agrupacion6.insert_many(df_6_pandas.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ecac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 7\n",
    "df_7 = df_enriched.groupby('Classification','DATE').agg(count(lit(1)).alias('Cant'))\n",
    "\n",
    "df_7_pandas =df_7.toPandas()\n",
    "db_mongo.Agrupacion7.insert_many(df_7_pandas.to_dict('records'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
